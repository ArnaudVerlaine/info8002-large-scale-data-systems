{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark Tutorial\n",
    "\n",
    "University of Li√®ge                                                      \n",
    "Joeri Hermans (joeri.hermans@doct.ulg.ac.be)                         \n",
    "\n",
    "This notebook will guide the students of \"Large-Scale Distributed Systems\" through their initial steps with [Apache Spark](https://spark.apache.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading, unpacking and moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'spark-2.2.0-bin-hadoop2.7': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Download the latest version of Apache Spark.\n",
    "!rm -r spark-2.2.0-bin-hadoop2.7.tgz\n",
    "!wget -q http://apache.tt.co.kr/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'apache-spark': No such file or directory\n",
      "tar (child): spark-2.2.0-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
      "tar (child): Error is not recoverable: exiting now\n",
      "tar: Child returned status 2\n",
      "tar: Error is not recoverable: exiting now\n",
      "total 20\n",
      "drwxr-xr-x  4 joeri joeri 4096 Nov 15 01:42 .\n",
      "drwxr-xr-x 10 joeri joeri 4096 Nov 15 00:36 ..\n",
      "drwxr-xr-x 12 joeri joeri 4096 Jul  1 01:09 apache-spark\n",
      "drwxr-xr-x  2 joeri joeri 4096 Nov 15 00:48 .ipynb_checkpoints\n",
      "-rw-r--r--  1 joeri joeri 2623 Nov 15 01:41 spark-tutorial.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Unpack the framework.\n",
    "!rm -r apache-spark\n",
    "!tar -xzf spark-2.2.0-bin-hadoop2.7.tgz\n",
    "!mv spark-2.2.0-bin-hadoop2.7 apache-spark\n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move it to the desired location. Make sure destination is writable.\n",
    "!rm -rf /opt/apache-spark\n",
    "!mv apache-spark /opt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, in order to use the libraries, make sure the libs are included in $PATH and $PYTHONPATH.\n",
    "# Add this to your .bashrc or .zshrc depending on your shell.\n",
    "!export SPARK_HOME=/opt/apache-spark\n",
    "!export PYTHONPATH=\"$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH\"\n",
    "!export PATH=\"$SPARK_HOME/bin:$PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Spark\n",
    "\n",
    "Test your installation by importing the Python module. If you did everything correctly, this should not produce any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections we give a basic introduction to most of the concepts in Spark, and how (not) to use them. If you whish to implement something specific, you can always refer to the [documentation](https://spark.apache.org/docs/latest/quick-start.html).\n",
    "\n",
    "As you saw in the lecture, Apache Spark provides two distinct concepts where computation resides. The first being the *driver*, which is basically the process which is responsible for issuing the tasks to the cluster, it is also used for *collecting* the results of the computation. The second, and final being the *executors*, which actually *execute* the tasks issues by the driver. Furthermore, an executor is also responsible for maintaining the state of the computation. For instance, if you issue a set of tasks from the driver process, and go for a coffee for instance, the (intermediate) result of the computation cannot be lost as long as the driver process is still alive.\n",
    "\n",
    "Depending on how the cluster is configured (stand-alone, YARN, Mesos, ...), a different master-address has to be specified. The master will basically allocate the resources you requested and assign them to the requestor. In general, you have to specify 4 things before starting your Spark Application, i.e., the `application_name`, the `master_address` or master-mode (e.g., yarn-client, in this case you request yarn to schedule your resources), `num_executors` which obviously defines the number of executors you whish to use, and finally, `num_processes` which is the number of child-processes (you can view them as threads) which are run in a single executor. This option is advantegeous in the sense that the child processes within a single executor will share the same memory. As a result, these processes can share data really quickly (instead of reading data from a different node on the network)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the name of your application.\n",
    "application_name = \"Large-Scale Distributed Systems Spark Tutorial\"\n",
    "\n",
    "# Define the master address, for local mode this is local[*].\n",
    "master = \"local[*]\"\n",
    "# Number of executors.\n",
    "num_executors = 2\n",
    "# Number of child processes per executors.\n",
    "num_processes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using these options, we are able to construct a `SparkConf` object which tells the cluster how to allocate the resources, and specify the behaviour of the cluster. In the configuration specified below, you use the variables which are defines above, including the amount of memory you require on an executor level. In general, Spark thrives on the availability of RAM (more data can be kept in memory, so it can be processed faster).\n",
    "\n",
    "A different option you can specify is a custom `Serializer`, the sole responsibility of the serializer is to serialize the memory to some kind of persistent medium whenever it doesn't fit in memory. Nevertheless, the way in-memory (heap) objects are serialized can differ from implementation. By default, Spark will use the Java serializer, which in some cases might not be sufficient because it fails to encode dependencies. That's why it is commonly a good-practice to use the Kryo Serializer. Furthermore, it is also faster than the Java serializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", `num_processes`)\n",
    "conf.set(\"spark.executor.instances\", `num_executors`)\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the list of options you can specify are endless, and are application-dependent in order to obtain a good throughput of your processing application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
