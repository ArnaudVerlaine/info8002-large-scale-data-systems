{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale Distributed Systems: SDSS Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For a summary of the project context, including the logistics of the project, please refer to the [project description](https://github.com/glouppe/info8002-large-scale-database-systems/raw/master/projects/project_analysis.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, in order to use the libraries, make sure the libs are included in $PATH and $PYTHONPATH.\n",
    "# Add this to your .bashrc or .zshrc depending on your shell.\n",
    "!export SPARK_HOME=/opt/apache-spark   # CHANGE DESTINATION PATH IF DESIRED.\n",
    "!export PYTHONPATH=\"$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH\"\n",
    "!export PATH=\"$SPARK_HOME/bin:$PATH\"\n",
    "\n",
    "# Usually this is done at a cluster level, so you should not have to worry about it.\n",
    "!export JAVA_OPTS=\"-Xms1024m -Xmx2048m\" # Adjust to your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will need this at a later stage, I will come back to this later.\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-avro_2.11:4.0.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the name of your application.\n",
    "application_name = \"Large-Scale Distributed Systems: SDSS\"\n",
    "\n",
    "# Define the master address, for local mode this is local[*].\n",
    "# If you don't want to use all cores on your machine please specify local[n].\n",
    "master = \"local[*]\"\n",
    "# Number of executors.\n",
    "num_executors = 1\n",
    "# Number of child processes per executors.\n",
    "num_processes = 2\n",
    "# Total number of parallel processes.\n",
    "num_workers = num_executors * num_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"spark.app.name\", application_name)\n",
    "conf.set(\"spark.master\", master)\n",
    "conf.set(\"spark.executor.cores\", str(num_processes))\n",
    "conf.set(\"spark.executor.instances\", str(num_executors))\n",
    "conf.set(\"spark.executor.memory\", \"4g\") # Adjust according to your requirements.\n",
    "conf.set(\"spark.locality.wait\", \"0\")\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Allocate a Spark session based on the provided configuration and application name.\n",
    "# Which may, or may not, already be created (getOrCreate()).\n",
    "spark = SparkSession.builder.config(conf=conf).appName(application_name).getOrCreate()\n",
    "# Create a shortcut for the SparkContext.\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the list of data-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned in the Spark Tutorial, a common beginner's mistake is to load all the data in the driver process, and then parallize the data to the executors. This is fine for smaller toy-problems, but obviously this doesn't scale to larger datasets. An alternative would be to apply the following idea:\n",
    "\n",
    "Assuming there is an underlying distributed filesystem, like HDFS or NFS, the absolute path of the data files remain identical across all other nodes. A possiblity would be to first obtain the paths of all data files, parallelize these to the executors, and let the executors read and parse the FITS files in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting a single FITS file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first investigate the structure of a single FITS file. A FITS file always consists of a set of HDU (Header Data Unit). However, some HDU's might be a table-like structure, an image, or some other binary format. The specific type can always be inferred from the HDU list as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "\n",
    "# Lets inspect the file structure of a single FITS file.\n",
    "hdulist = fits.open(plates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the information on all HDU's in the FITS file.\n",
    "hdulist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the header of the 'BinTableHDU'.\n",
    "hdulist[1].header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Close the HDU access handle.\n",
    "hdulist.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing FITS files with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only interested in a single state of an observation, we filter all the plates plates which have been recorded more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get plates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize all unique plates to the executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parallelize plates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will apply a lambda function handling all paths that have been allocated to a single partition. In this partion, we iterate over every provided absolute path, and process every records associated with the data table. Furthermore, we only extract the required data that we need to solve the questions posed below. Nevertheless, there are some special cases that we need to be aware of. For instance, every fiber has an associated `warning` level. To improve the quality of the analysis, we only select the fibers which have no warnings assigned to them, i.e., `ZWARNING = 0` [SDSS ZWARNING description](http://www.sdss3.org/dr8/algorithms/bitmask_zwarning.php). Afterwards, we convert the RDD to a *DataFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_plates(iterator):\n",
    "    # Iterate through all plates in the current iterator.\n",
    "    for path in iterator:\n",
    "        # Open the specified FITS file.\n",
    "        hdulist = fits.open(path)\n",
    "        # Iterate through all records in the HDU.\n",
    "        for record in hdulist[1].data:\n",
    "            # Check if the current object has spectra problems.\n",
    "            if record['ZWARNING'] == 0:\n",
    "                d = {}\n",
    "                # Extract the desired columns, and convert them to a more favourable type.\n",
    "                d['plate']          = int(record['PLATE'])\n",
    "                d['tile']           = int(record['TILE'])\n",
    "                d['mjd']            = int(record['MJD'])\n",
    "                d['fiber']          = int(record['FIBERID'])\n",
    "                d['ra']             = float(record['PLUG_RA'])\n",
    "                d['dec']            = float(record['PLUG_DEC'])\n",
    "                d['class']          = record['CLASS']\n",
    "                d['subclass']       = record['SUBCLASS']\n",
    "                d['z']              = float(record['Z'])\n",
    "                d['z-err']          = float(record['Z_ERR'])\n",
    "                d['spectro-flux']   = record['SPECTROFLUX'].tolist()\n",
    "                yield Row(**d)\n",
    "        hdulist.close()\n",
    "\n",
    "# Read a DataFrame of objects from the plate paths RDD.\n",
    "objects = # Read plates and convert to DataFrame.\n",
    "\n",
    "objects.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of objects without problems: \" + str(objects.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip:** It might be useful to write the dataframe to disk, this would prevent you from having to parse all the individual FITS files (which eventually benefits the query performance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Question 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show you what an example use-case would be, let us assume that we are interested in the stars which are moving towards us. Meaning, they are *blueshifted*, i.e., $z < 0$. This particular information could be used to set-up a survey which is interested in answering the question whether those particular stars might *one-day* disturb the stability of our solar-system. Therefore, we want to extract their coordinates (in terms of Right Ascension and Declination) fur further, and more detailed observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects.where('class = \"STAR\"').where('z < 0').select('ra', 'dec').distinct().show()\n",
    "\n",
    "# Instead of .show() you could call .collect() which returns a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "*Looking at galaxies, is the expansion of the Universe similar across all regions of the sky? Meaning, is the redshift of the galaxies about equal across the sky?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Make a plot with the collected data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "*Is the expansion of the Universe accelerating?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Compute the acceleration, and make a plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "*What is the average velocity of the galaxies which are redshifted?*\n",
    "\n",
    "However, for large redshifts (~ z >= 1) it does not make sense to use the traditional equations as in [http://www.ifa.hawaii.edu/users/acowie/class05/home9_sol.html](http://www.ifa.hawaii.edu/users/acowie/class05/home9_sol.html). Therefore, one needs to take relativistic effects into account. As a tip, it might be interested to read these pages on Wikipedia: [https://en.wikipedia.org/wiki/Peculiar_velocity](Peculiar Velocity) and [https://en.wikipedia.org/wiki/Recessional_velocity](Recessional Velocity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "*What is the average velocity of the quasars which are redshifted?*\n",
    "\n",
    "Relativistic effects are very important here! Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "*Are there galaxies with a relatively small flux which are blueshifted?*\n",
    "\n",
    "**Tip**: [https://en.wikipedia.org/wiki/Absolute_magnitude](Absolute Magnitude) & [https://en.wikipedia.org/wiki/Apparent_magnitude](Apparent Magnitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectro_flux_to_magnitude(row):\n",
    "    \"\"\"Converts the spectro-flux filter array to magnitudes\n",
    "    according to:\n",
    "    \n",
    "    SPECTROFLUX 5-element array of integrated flux in each of the\n",
    "    5 SDSS imaging filters (ugriz); the units are nanomaggies,\n",
    "    which is 1 at 22.5 magnitude; convert to magnitudes with\n",
    "    22.5 - 2.5 * LOG_10(SPECTROFLUX);\n",
    "    \n",
    "    The u-band and z-band counts are meaningless and should not be used.\n",
    "    \n",
    "    So, we apply said equation by taking the average of the gri filters.\n",
    "    \"\"\"\n",
    "    d = row.asDict()\n",
    "    spectroflux = d['spectro-flux']\n",
    "    d['hasflux'] = (np.count_nonzero(spectroflux) > 0) # Why is this important?\n",
    "    averaged_flux = np.abs(np.average(spectroflux[1:4]))\n",
    "    magnitude = 22.5 - 2.5 * np.log10(averaged_flux)\n",
    "    del d['spectro-flux']\n",
    "    d['magnitude'] = float(magnitude)\n",
    "    \n",
    "    return Row(**d)\n",
    "\n",
    "\n",
    "# Code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "*What is the distribution of the spectral type of all observed stars?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a histogram for all spectral types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "*How does the query performance scale when tuning the number of partitions, relative to the amount of CPU cores that are available? *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "*Your conclusion here.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary with plots describing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
